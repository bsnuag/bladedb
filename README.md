# bladedb [![Go Report Card](https://goreportcard.com/badge/github.com/bsnuag/bladedb)](https://goreportcard.com/report/github.com/bsnuag/bladedb)

**`bladedb` is a persistent, KV store written in go**

## Motivation

After learning distributed systems (especially databases), it's internals, I started writing bladedb to give a shape to my learning. The design and implementations are derived from many production ready databases & books.  

## Usage
- To start using `bladedb`, install Go 1.12 or above and run 
```go get bladedb```

- Use it in embedded mode
    - Write to bladedb 
    
      ```err := bladedb.Put(keyByte, valueByte)``` 
    - Read from bladedb
    
       ```err = bladedb.Get(keyByte)```
   - Delete from bladedb
   
        ```delValue, err := bladedb.Delete(keyByte)```
## Implementation

Overview of bladedb implementation - 

- Sharding: 
    - By default data is divided into 64 (configurable) shards. 
    - Each key (K) belong to one shard, decided using hash algorithm. 
    - Each shard has it's own components (storage, index) isolated from others.   
- Data Distribution: 
    - Data distribution happened by the concept of sharding, by design it supports distribution of shards across nodes. 
- Storage: 
    - It uses `LSM` tree concept 
    - `WAL` files are used for atomicity across writes (Gets flushed periodically) 
    - Data is written to `SSTable` once `WAL` reaches a threshold or when a manual flush is triggered  
- Cache:
    - `bladedb` relies on OS for caching 
- Compaction: 
    - Uses `levelled compaction` strategy
    - At most one compaction per shard
- Index: 
    - `Bladedb` is designed for ``ssd`` devices   
    - An index is build out of SSTables when db starts
    - The key (K) is hashed using sha-256 algorithm which generates 32 byte hashed data - Chances of [collision](https://crypto.stackexchange.com/questions/47809/why-havent-any-sha-256-collisions-been-found-yet) is very less  
    - Each index entry takes about 44 bytes (32-hash + sstable-offset-details + timestamp). Roughly _22.7 Millions keys cab be stored in 1 GB of memory_ 
- Thread Safety: 
    - `bladedb` is thread safe (uses RW lock), parallelism is defined by number of shards for write operations
    - Lock is implemented per shard per record level

## Future Work

- Making it Distributed: Use Raft consensus algorithm to make it distributed KV Store
- Scan: As on today it supports basic operations like Get, Put and Delete 
- Data Compression & Checksum
- Backup & Recovery
- Monitoring Stats - Stats like Disk, Memory, CPU, Compactions
- A tool like nodetool (used in cassandra) to start, stop, get stats etc.   
- Optimisation: With initial implementation many optimisations were not achieved. Below are few:
    1. Replace in-memory skiplist (used to store active mem tables) with off-heap data structure to avoid GC cycles   
    2. Moving in-memory Index to Off-Heap to reduce effects of GC (Stop the world event) - Low memory usage, would work in systems running with less memory 
    3. Replacing value from in-memory sstable (data which is not flushed yet) with WAL file offset details - Performance evaluation is needed
    4. Reduce Disk Space utilization during compaction - A particular SSTable can be deleted immediately during compaction once processing completes for it, instead waiting for entire compaction to complete - Inspired by Scylla Hybrid compaction strategy
    5. Controller for memflush & compaction workers to achieve stable read,write latency    
    
## Configuration

It allows users to override config parameters - 

Parameter | Default  | Comment
--- | --- | ---
log-flush-interval | 10 sec | frequency in which wal file data gets flushed to disk. When value is too high - data loss chances increases, when too low - performance will degrade
partitions | 64 | number of shards
compact-worker | 8 | number of compaction can be executed in parallel (one compaction per shard)
memflush-worker | 8 | number of memflush can be executed in parallel (low value for write load use may increase memory usage) 

## Benchmarks

- Benchmarks numbers were generated by running db in embedded mode.   
- A simple utility comes with bladedb to perform benchmark, run ```go run example/benchmark/benchEmbedded.go --help``` to see supported options 
```
  -kSz int
    	key size in bytes (default 256)
  -nR int
    	number of reads (default 40000000)
  -nThreads int
    	number of clients (default 8)
  -nW int
    	number of writes (default 40000000)
  -r	simulate read only
  -rw
    	simulate read write concurrently
  -vSz int
    	value size in bytes (default 256)
  -w	simulate write only
```  
- For read-write (`-rw option`) type benchmark, nThreads are divided equally and it doesn't wait for write to complete before read starts
- For read (`-r option`), data need to be ingested before running benchmark utility

**Config** | **Duration (Sec)**
--------------|--------------
``go run example/benchmark/benchEmbedded.go -nThreads=8 -nW=40000000 -nR=40000000 -w -kSz=256 -vSz=256`` | 185
``go run example/benchmark/benchEmbedded.go -nThreads=16 -nW=40000000 -nR=40000000 -r -kSz=256 -vSz=256`` | 115
``go run example/benchmark/benchEmbedded.go -nThreads=8 -nW=40000000 -nR=40000000 -w -kSz=512 -vSz=512`` | 338
``go run example/benchmark/benchEmbedded.go -nThreads=16 -nW=40000000 -nR=40000000 -r -kSz=512 -vSz=512`` | 205
``go run example/benchmark/benchEmbedded.go -nThreads=8 -nW=40000000 -nR=40000000 -w -kSz=1024 -vSz=1024`` | 605
``go run example/benchmark/benchEmbedded.go -nThreads=16 -nW=40000000 -nR=40000000 -r -kSz=1024 -vSz=1024`` | 390


**Note:** 
- Benchmark completes by flushing all active mem tables to disk, any possible compaction
- Above benchmark measurement are from `Macbook pro` configured with `16 GB 1600 MHz DDR3 RAM`, `2.2 GHz Quad-Core Intel Core i7 processor` and  `4 cores with hyper threading enabled`
- DB Config used during benchmark -  
```
data-dir: /tmp/bladedb.data/
log-dir: /tmp/bladedb.log/
log-flush-interval: 10
partitions: 8
compact-worker: 2
memflush-worker: 2
log-level: info
client-listen-port: 9099
``` 
   